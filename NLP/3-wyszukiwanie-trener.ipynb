{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Wyszukiwanie informacji</h1>\n",
    "\n",
    "Jednym z zadań nlp jest wyszukiwanie dokumentów podobnych dla danego zapytania (wyszukiwarki internetowe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weźmy korpus dokumentów, jak poniżej. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\"Romeo and Juliet\",\n",
    "         \"Juliet: O happy dagger\",\n",
    "         \"Romeo died by dagger\",\n",
    "         \"'Live free or die', that’s the New-Hampshire’s motto\",\n",
    "         \"Did you know, New-Hampshire is in New-England\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE: Zadajemy zapytanie \"dies, dagger\". Prosze uszeregować dokumenty w kolejności od najbardziej pasujących. \n",
    "\n",
    "Transformujemy query do naszej reprezentacji i liczymy ile słów się pokrywa (mnożenie wektorów i sumowanie)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = [\"dies\", \"dagger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'romeo': 10, 'juliet': 5, 'happi': 4, 'dagger': 1, 'die': 2, \"'live\": 0, 'free': 3, '’': 11, 'new-hampshir': 9, 'motto': 7, 'know': 6, 'new-england': 8}\n",
      "[[0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 1 1 0 0 0 1 0 1 0 2]\n",
      " [0 0 0 0 0 0 1 0 1 1 0 0]]\n",
      "[[0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def my_tokenizer(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    stemmer = PorterStemmer()\n",
    "    res = [stemmer.stem(word) for word in tokens]\n",
    "    return res \n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer = my_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "data = vectorizer.transform(corpus).todense()\n",
    "print(data)\n",
    "\n",
    "query_data = vectorizer.transform(query).todense()\n",
    "print(query_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0]], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "res = np.sum(np.dot(data, query_data.T), axis=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Żeby dokumenty były podobne to muszą zawierać te same słowa.\n",
    "\n",
    "ZADANIE: Czy aby na pewno 2 i 4 są tak samo podobne do query?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można użyć transformaty tfidf: https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n",
    "\n",
    "tf - waga słowa jest proporcjonalna do ilości jego wystąpień w dokumencie\n",
    "\n",
    "idf - waga jest odwrotnie proporcjonalna do ilości dokumentów w jakich występuje\n",
    "\n",
    "Intuicyjnie tfidf zmniejsza wagi często występujących słów, co powoduje że są mniej ważne (potencjalnie można nie usuwać stopwords..)\n",
    "\n",
    "ZADANIE: Proszę spróbować zaimplementować własne tfidf (https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE: Użyjmy Tfidf z sklearn i dokonajmy wyszukiwania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'romeo': 10, 'juliet': 5, 'happi': 4, 'dagger': 1, 'die': 2, \"'live\": 0, 'free': 3, '’': 11, 'new-hampshir': 9, 'motto': 7, 'know': 6, 'new-england': 8}\n",
      "[[ 0.          0.          0.          0.          0.          0.70710678\n",
      "   0.          0.          0.          0.          0.70710678  0.        ]\n",
      " [ 0.          0.53177225  0.          0.          0.659118    0.53177225\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.57735027  0.57735027  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.57735027  0.        ]\n",
      " [ 0.34706676  0.          0.28001128  0.34706676  0.          0.          0.\n",
      "   0.34706676  0.          0.28001128  0.          0.69413353]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.61418897  0.          0.61418897  0.49552379  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=my_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "data = vectorizer.transform(corpus)\n",
    "print(data.todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ],\n",
       "        [ 0.53177225],\n",
       "        [ 1.15470054],\n",
       "        [ 0.28001128],\n",
       "        [ 0.        ]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data = vectorizer.transform(query)\n",
    "print(query_data.todense())\n",
    "\n",
    "res = np.sum(np.dot(data, query_data.T).todense(),axis=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE: Co spowodowało że podobieństwo zostało rozróżnione?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
