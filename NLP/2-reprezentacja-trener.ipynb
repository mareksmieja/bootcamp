{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Wektorowa reprezentacja tekstu</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W najprostszym przypadku budowanie reprezentacji wektorowej polega na zliczaniu ilości słów (ciągów). Każde słowo to atrybut a wartość w danym atrybucie to ilość wystąpniń tego słowa. \n",
    "\n",
    "\n",
    "Najprostsza reprezentacja to reprezentacja bag-of-words: https://en.wikipedia.org/wiki/Bag-of-words_model. Kazdy wektor zapisany jest w rzadkiej reprezentacji jako że słownik może być duży a dokument mały."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE: Weźmy tekst i podzielmy na tokeny. Proszę stworzyć słownik słów oraz policzyć częstości każdego słowa. Można użyć Counter - daje nam słownik: https://docs.python.org/2/library/collections.html#collections.Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\"\n",
    "\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----słownik----\n",
      "{'large', 'a', '.', 'that', 'files', 'into', 'in', 'corpus', 'program', 'unigrams', 'I', 'trigrams', 'NLTK', 'fourgrams', 'to', ')', 'breaks', 'bigrams', 'of', 'collection', ',', '(', 'and', 'fivegrams', 'need', 'txt', 'write'}\n",
      "-----licznik----\n",
      "Counter({'a': 5, ',': 3, 'I': 2, 'need': 2, 'to': 2, 'write': 2, 'program': 2, 'in': 2, 'NLTK': 2, 'that': 2, 'breaks': 2, 'corpus': 2, '(': 1, 'large': 1, 'collection': 1, 'of': 1, 'txt': 1, 'files': 1, ')': 1, 'into': 1, 'unigrams': 1, 'bigrams': 1, 'trigrams': 1, 'fourgrams': 1, 'and': 1, 'fivegrams': 1, '.': 1})\n",
      "-----klucze---\n",
      "dict_keys(['I', 'need', 'to', 'write', 'a', 'program', 'in', 'NLTK', 'that', 'breaks', 'corpus', '(', 'large', 'collection', 'of', 'txt', 'files', ')', 'into', 'unigrams', ',', 'bigrams', 'trigrams', 'fourgrams', 'and', 'fivegrams', '.'])\n",
      "----wartości--\n",
      "dict_values([2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1])\n",
      "--miejsce--\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "unigrams = set(tokens)\n",
    "print(\"-----słownik----\")\n",
    "print(unigrams)\n",
    "\n",
    "\n",
    "\n",
    "print(\"-----licznik----\")\n",
    "counts1 = Counter(tokens)\n",
    "print(counts1)\n",
    "print(\"-----klucze---\")\n",
    "print(counts1.keys())\n",
    "print(\"----wartości--\")\n",
    "print(counts1.values())\n",
    "print(\"--miejsce--\")\n",
    "print(counts1['I'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE: Zostańmy przy unigramach. Proszę napisać reprezentację bag-of-words. W tym celu:\n",
    "\n",
    "-tworzymy słownik słów na podstawie zdanego tekstu (metoda fit)\n",
    "\n",
    "-dla stokenizowanego dokumentu, tworzymy macierz częstości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'large', 'a', '.', 'that', 'files', 'into', 'in', 'corpus', 'program', 'unigrams', 'I', 'trigrams', 'NLTK', 'fourgrams', 'to', ')', 'breaks', 'bigrams', 'of', 'collection', ',', '(', 'and', 'fivegrams', 'need', 'txt', 'write'}\n",
      "[ 1.  5.  1.  2.  1.  1.  2.  2.  2.  1.  2.  1.  2.  1.  2.  1.  2.  1.\n",
      "  1.  1.  3.  1.  1.  1.  2.  1.  2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(tokens):\n",
    "    words = set(token)\n",
    "    return words\n",
    "\n",
    "def transform(sentence_words, words):\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words))\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)\n",
    "\n",
    "words = fit(tokens)\n",
    "print(words)\n",
    "representation = transform(tokens, words)\n",
    "print(representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teksto można reprezentaować też jako ciągi - co daje reprezentacja tekstu przez dłuższe ciągi? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('I', 'need'): 2, ('need', 'to'): 2, ('to', 'write'): 2, ('write', 'a'): 2, ('a', 'program'): 2, ('program', 'in'): 2, ('in', 'NLTK'): 2, ('NLTK', 'that'): 2, ('that', 'breaks'): 2, ('breaks', 'a'): 2, ('a', 'corpus'): 2, ('corpus', '('): 1, ('(', 'a'): 1, ('a', 'large'): 1, ('large', 'collection'): 1, ('collection', 'of'): 1, ('of', 'txt'): 1, ('txt', 'files'): 1, ('files', ')'): 1, (')', 'into'): 1, ('into', 'unigrams'): 1, ('unigrams', ','): 1, (',', 'bigrams'): 1, ('bigrams', ','): 1, (',', 'trigrams'): 1, ('trigrams', ','): 1, (',', 'fourgrams'): 1, ('fourgrams', 'and'): 1, ('and', 'fivegrams'): 1, ('fivegrams', '.'): 1, ('.', 'I'): 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"I need to write a program in NLTK that breaks a corpus (a large collection of txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams. I need to write a program in NLTK that breaks a corpus\"\n",
    "token = nltk.word_tokenize(text)\n",
    "\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)\n",
    "\n",
    "counts2 = Counter(bigrams)\n",
    "print(counts2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do budowy reprezentacji używamy sklearn do tej samej czynności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\", \"I'm 20 years old.\"]\n",
      "{'hello': 9, 'mr': 12, 'smith': 18, 'how': 10, 'are': 2, 'you': 23, 'doing': 6, 'today': 20, 'the': 19, 'weather': 21, 'is': 11, 'great': 8, 'and': 1, 'python': 15, 'awesome': 3, 'sky': 17, 'pinkish': 14, 'blue': 4, 'shouldn': 16, 'eat': 7, 'cardboard': 5, '20': 0, 'years': 22, 'old': 13}\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 23)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 11)\t2\n",
      "  (1, 15)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 21)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 19)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 23)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 22)\t1\n",
      "[[0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 1 0 1 0 0 0 0 1 0 0 2 0 0 0 1 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard. I'm 20 years old.\"\n",
    "sentences = sent_tokenize(EXAMPLE_TEXT)\n",
    "\n",
    "print(sentences)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences)\n",
    "print( vectorizer.vocabulary_ )\n",
    "bag_of_words = vectorizer.transform(sentences)\n",
    "print(bag_of_words)\n",
    "print(bag_of_words.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE: Możemy tutaj użyć preprocessing i tokenizacje domyślną, ale można też zdefiniować własną. Proszę użyć wcześniej zdefiniowane dunkcje do przetwarzania tekstu w naszym transfromerze - zobacz: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome.\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n",
      "I'm 20 years old.\n",
      "{'hello': 7, 'mr.': 8, 'smith': 14, 'today': 15, 'weather': 16, 'great': 6, 'python': 12, 'awesom': 3, 'sky': 13, 'pinkish-blu': 11, \"n't\": 9, 'eat': 5, 'cardboard': 4, 'I': 2, \"'m\": 0, '20': 1, 'year': 17, 'old': 10}\n",
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome.\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n",
      "I'm 20 years old.\n",
      "[[0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#TODO wykorzystać poprzednie\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#tokenizer działa na tokenach już\n",
    "def stemming_tokenizer(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in tokenized]\n",
    "\n",
    "#preprocessor działa na całym dokumencie\n",
    "def my_preprocessing(word):\n",
    "    print(word)\n",
    "    return word\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = my_preprocessing, tokenizer=stemming_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "print( vectorizer.vocabulary_ )\n",
    "print(vectorizer.transform(sentences).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer buduje nam słownik, czyli definiuje poszczególne wymiary. Mając taki słownik możemy transfromować nowe zdania.\n",
    "\n",
    "ZADANIE: stworzyć słownik w oparciu o zadany tekstu. Następnie wziąć dokument który zawiera słowa niebędące w słowniku - jak wygląda jego reprezentacja?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ms. smith\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence = [\"hello ms. smith\"]\n",
    "vectorizer.transform(new_sentence).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZADANIE (dla chętnych): Jeśli będzie czas to można zobaczyć na inne vektoryzery: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html. Można też spróbować wykorzystać POS do tworzenia reprezentacji: https://stackoverflow.com/questions/24002485/python-how-to-use-pos-part-of-speech-features-in-scikit-learn-classfiers-svm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
